{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sparknlp\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import databricks.koalas as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intermediate-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-insider",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder.appName(\"Spark NLP\").master(\"local[4]\").config(\"spark.driver.memory\",\"16G\").\\\n",
    "                    config(\"spark.driver.maxResultSize\", \"0\").config(\"spark.kryoserializer.buffer.max\", \"2000M\").\\\n",
    "                    config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-blogger",
   "metadata": {},
   "source": [
    "# 1 Initiate a Spark session with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liable-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "composed-reply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.0.1\n",
      "Apache Spark version:  3.1.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flush-enough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onto_recognize_entities_sm download started this may take some time.\n",
      "Approx size to download 160.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "#pipeline = PretrainedPipeline(\"recognize_entities_dl\", \"en\")\n",
    "pipeline = PretrainedPipeline('onto_recognize_entities_sm', lang = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "super-baker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRODUCT', 'O', 'O', 'O']\n",
      "['Google', 'TensorFlow']\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.annotate('Google has announced the release of a beta version of the popular TensorFlow machine learning library.')\n",
    "#print(result['ner'])\n",
    "print(result['ner'])\n",
    "print(result['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stuffed-maine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_sentiment download started this may take some time.\n",
      "Approx size to download 4.9 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline('analyze_sentiment', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "senior-evening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'negative', 'negative']\n",
      "['This', 'is', 'a', 'very', 'boring', 'movie', '.', 'I', 'recommend', 'others', 'to', 'avoid', 'this', 'movie', 'is', 'not', 'good', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.annotate('This is a very boring movie. I recommend others to awoid this movie is not good..')\n",
    "#result\n",
    "print(result['sentiment'])\n",
    "print(result['checked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-bibliography",
   "metadata": {},
   "source": [
    "#### The word awoid has been corrected to avoid by spell checker insdie this pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-ranking",
   "metadata": {},
   "source": [
    "# 2. Document Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "technical-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accompanied-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start() # start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "qualified-sixth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.0.1\n",
      "Apache Spark version:  3.1.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "disabled-option",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Hello, this is an...|\n",
      "|And this is a sec...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen = [['Hello, this is an example sentence'],['And this is a second sentence.']]\n",
    "# spark is the Spark Session automatically started by pyspark.\n",
    "spark_df = spark.createDataFrame(sen, ['text'])\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "integrated-marketing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "intellectual-outside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|            document|\n",
      "+--------------------+--------------------+\n",
      "|Hello, this is an...|[{document, 0, 33...|\n",
      "|And this is a sec...|[{document, 0, 29...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler().setInputCol('text').setOutputCol('document')\n",
    "doc_df=documentAssembler.transform(spark_df)\n",
    "doc_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caring-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "emotional-stranger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['Hello, this is an example sentence']),\n",
       " Row(result=['And this is a second sentence.'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df.select('document.result').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-mason",
   "metadata": {},
   "source": [
    "## ------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accurate-humor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---+--------------------+---------------+----------+\n",
      "|annotatorType|begin|end|              result|       metadata|embeddings|\n",
      "+-------------+-----+---+--------------------+---------------+----------+\n",
      "|     document|    0| 33|Hello, this is an...|{sentence -> 0}|        []|\n",
      "|     document|    0| 29|And this is a sec...|{sentence -> 0}|        []|\n",
      "+-------------+-----+---+--------------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "doc_df.withColumn(\"tmp\", F.explode(\"document\")).select('tmp.*').show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "vertical-appendix",
   "metadata": {},
   "source": [
    "annotations_df.select(\"token\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cellular-neutral",
   "metadata": {},
   "source": [
    "from sparknlp import Finisher\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "finisher = Finisher().setInputCols([\"token\", \"stems\", \"pos\"])\n",
    "explain_pipeline_model = PretrainedPipeline(\"explain_document_ml\").model\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "arbitrary-honolulu",
   "metadata": {},
   "source": [
    "pipeline = Pipeline().setStages([explain_pipeline_model, finisher])\n",
    "sentences = [['Hello, this is an example sentence'], ['And this is a second sentence.']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "honest-disabled",
   "metadata": {},
   "source": [
    "data = spark.createDataFrame(sentences).toDF(\"text\")\n",
    "model = pipeline.fit(data)\n",
    "annotations_finished_df = model.transform(data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "subjective-reset",
   "metadata": {},
   "source": [
    "annotations_finished_df.select('finished_token').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-killer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
