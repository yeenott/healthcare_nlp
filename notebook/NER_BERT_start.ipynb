{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GsDDSmTM6N9"
   },
   "source": [
    "# NER Pipeline with BERT and Glove Embedding in Spark NLP\n",
    "\n",
    "In this tutorial, we demostrate how to building a NER pipeline using Bert and Glove Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "1. [Blog](https://towardsdatascience.com/named-entity-recognition-ner-with-bert-in-spark-nlp-874df20d1d77)\n",
    "2. [Models Hub](https://nlp.johnsnowlabs.com/models?q=bert&task=Embeddings)\n",
    "3. [Original Code](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/blogposts/3.NER_with_BERT.ipynb)\n",
    "4. [Webinar](https://events.johnsnowlabs.com/webinar-state-of-the-art-named-entity-recognition-using-bert?hsCtaTracking=c5bfb98f-bf4e-4b81-92d0-4ad149f9da05%7C47b0af91-4c83-401d-9163-2417863ed82b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "3TZw9vE5M6OH"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRhmsIcCM6OH",
    "outputId": "0b3a1f3a-74b1-402e-a5d9-b909965f1a1f"
   },
   "source": [
    "# This is only to setup PySpark and Spark NLP on Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylCinY-tM6OI"
   },
   "source": [
    "## 1. Import libraries and download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A6rnibyVM6OI"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9mD4BObM6OI",
    "outputId": "2fca5cad-5f40-4539-f402-38f21ca90711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.0.1\n",
      "Apache Spark version:  3.1.1\n"
     ]
    }
   ],
   "source": [
    "# if you have GPU\n",
    "# spark = sparknlp.start(gpu=True)\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dataset summary\n",
    "\n",
    "The [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) concerns language-independent named entity recognition task. It concentrates on four types of named entities: persons (PER), locations (LOC), organizations (ORG) and names of miscellaneous entities that do not belong to the previous three groups. The CoNLL-2003 shared task data files contain four columns separated by a single space. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2 tagging scheme, whereas the original dataset uses IOB1.\n",
    "\n",
    "We can annotate your own data in CONLL and then train a custom NER in Spark NLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnfsIM1MM6OJ",
    "outputId": "1906b933-04b2-43f3-d337-7a8bfa6e2104"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('eng.testa', <http.client.HTTPMessage at 0x7ff38dd9b490>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "urlretrieve('https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.train',\n",
    "           'eng.train')\n",
    "\n",
    "urlretrieve('https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.testa',\n",
    "           'eng.testa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ksp7t3WKM6OK",
    "outputId": "b50fdab5-aa1d-4239-f7d4-505e5ee3079f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART- -X- -X- O\n",
      "\n",
      "EU NNP B-NP B-ORG\n",
      "rejects VBZ B-VP O\n",
      "German JJ B-NP B-MISC\n",
      "call NN I-NP O\n",
      "to TO B-VP O\n",
      "boycott VB I-VP O\n",
      "British JJ B-NP B-MISC\n",
      "lamb NN I-NP O\n",
      ". . O O\n",
      "\n",
      "Peter NNP B-NP B-PER\n",
      "Black\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng.train\") as f:\n",
    "    c=f.read()\n",
    "\n",
    "print (c[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sNhRXZ1M6OK"
   },
   "source": [
    "## 2. Building Bert NER pipeline\n",
    "\n",
    "### 2.1. Reading annotated data into Spark NLP CoNLL reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOgifXmgM6OL",
    "outputId": "ab445474-a0e2-4d6e-87df-9875762f211e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|EU rejects German...|[{document, 0, 47...|[{document, 0, 47...|[{token, 0, 1, EU...|[{pos, 0, 1, NNP,...|[{named_entity, 0...|\n",
      "|     Peter Blackburn|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 4, Pe...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|\n",
      "| BRUSSELS 1996-08-22|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, BR...|[{pos, 0, 7, NNP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "\n",
    "training_data = CoNLL().readDataset(spark, './eng.train')\n",
    "training_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6R1bF9QM6OL",
    "outputId": "24217c54-e4b9-4885-8f47-beb3d89b6f31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14041"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              result|          embeddings|\n",
      "+--------------------+--------------------+\n",
      "|[B-ORG, O, B-MISC...|[[], [], [], [], ...|\n",
      "|      [B-PER, I-PER]|            [[], []]|\n",
      "|          [B-LOC, O]|            [[], []]|\n",
      "|[O, B-ORG, I-ORG,...|[[], [], [], [], ...|\n",
      "|[B-LOC, O, O, O, ...|[[], [], [], [], ...|\n",
      "|[O, O, O, O, O, O...|[[], [], [], [], ...|\n",
      "|[O, O, O, O, O, O...|[[], [], [], [], ...|\n",
      "|[O, O, O, O, O, O...|[[], [], [], [], ...|\n",
      "|[B-PER, O, B-MISC...|[[], [], [], [], ...|\n",
      "|[O, B-PER, O, O, ...|[[], [], [], [], ...|\n",
      "|[B-MISC, O, O, B-...|[[], [], [], [], ...|\n",
      "|                 [O]|                [[]]|\n",
      "|[O, B-LOC, O, B-L...|[[], [], [], [], ...|\n",
      "|[O, B-ORG, O, O, ...|[[], [], [], [], ...|\n",
      "|[O, O, O, O, O, O...|[[], [], [], [], ...|\n",
      "|[B-MISC, O, O, O,...|[[], [], [], [], ...|\n",
      "|[O, O, O, O, O, O...|[[], [], [], [], ...|\n",
      "|[B-LOC, O, O, O, ...|[[], [], [], [], ...|\n",
      "|[B-LOC, O, O, O, ...|[[], [], [], [], ...|\n",
      "|[O, O, O, O, O, O...|[[], [], [], [], ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.select(\"label.result\",\"label.embeddings\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVs0qpz7M6OM",
    "outputId": "f52045e1-1217-4a29-9c65-d804da451b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|CRICKET - LEICEST...|[{document, 0, 64...|[{document, 0, 64...|[{token, 0, 6, CR...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|\n",
      "|   LONDON 1996-08-30|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 5, LO...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|\n",
      "|West Indian all-r...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 3, We...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "\n",
    "test_data = CoNLL().readDataset(spark, './eng.testa')\n",
    "test_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPHBKA1FM6OL"
   },
   "source": [
    "### 2.2. Loading Bert Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4IA9Rn0M6OL"
   },
   "source": [
    "BertEmbeddings() annotator will take sentence and token columns and populate Bert embeddings in bert column, where each word is translated to a 768-dimensional vector. \n",
    "\n",
    "More Bert models can be found at [Models Hub](https://nlp.johnsnowlabs.com/models?q=bert&task=Embeddings). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-HV69OEM6OM",
    "outputId": "f433883e-244c-4835-c231-116c7458c1cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L2_128 download started this may take some time.\n",
      "Approximate size to download 16.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# we use BERT Tiny\n",
    "bert_annotator = BertEmbeddings.pretrained('small_bert_L2_128', 'en') \\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"bert\")\\\n",
    " .setBatchSize(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Bert Embeddings to the test dataset, so it can be compared with training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVs0qpz7M6OM",
    "outputId": "f52045e1-1217-4a29-9c65-d804da451b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|                bert|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|CRICKET - LEICEST...|[{document, 0, 64...|[{document, 0, 64...|[{token, 0, 6, CR...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|   LONDON 1996-08-30|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 5, LO...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|West Indian all-r...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 3, We...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = bert_annotator.transform(test_data)\n",
    "test_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3250"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving test data into local disk for future model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mM-nQblLPkPf"
   },
   "outputs": [],
   "source": [
    "# let's transform and save our test dataset for evaluation\n",
    "test_data.write.parquet(\"test_withEmbeds.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vufW7YRM6ON",
    "outputId": "013e8593-c692-4b8b-f939-fd0a4b3d5473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              result|          embeddings|              result|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[cricket, -, leic...|[[-1.6099558, 0.5...|[O, O, B-ORG, O, ...|\n",
      "|[london, 1996-08-30]|[[-0.66074246, 0....|          [B-LOC, O]|\n",
      "|[west, indian, al...|[[-1.2108909, 0.9...|[B-MISC, I-MISC, ...|\n",
      "|[their, stay, on,...|[[-0.93976337, 0....|[O, O, O, O, O, O...|\n",
      "|[after, bowling, ...|[[-1.1267805, 1.1...|[O, O, B-ORG, O, ...|\n",
      "|[trailing, by, 21...|[[-1.8359267, 0.4...|[O, O, O, O, B-OR...|\n",
      "|[essex, ,, howeve...|[[-1.2150192, 0.2...|[B-ORG, O, O, O, ...|\n",
      "|[hussain, ,, cons...|[[-1.6078961, 0.5...|[B-PER, O, O, O, ...|\n",
      "|[by, the, close, ...|[[-1.8683753, 1.1...|[O, O, O, B-ORG, ...|\n",
      "|[at, the, oval, ,...|[[-1.8740947, 0.6...|[O, O, B-LOC, O, ...|\n",
      "|[he, was, well, b...|[[-1.660714, 1.22...|[O, O, O, O, O, B...|\n",
      "|[derbyshire, kept...|[[-1.1823795, 0.2...|[B-ORG, O, O, O, ...|\n",
      "|[australian, tom,...|[[-1.1873059, 2.1...|[B-MISC, B-PER, I...|\n",
      "|[after, the, frus...|[[-1.841507, 1.30...|[O, O, O, O, O, O...|\n",
      "|[they, were, held...|[[-1.2608186, 0.8...|[O, O, O, O, O, O...|\n",
      "|[by, stumps, kent...|[[-1.6587898, 1.5...|[O, O, B-ORG, O, ...|\n",
      "|[cricket, -, engl...|[[-1.5593712, 0.2...|[O, O, B-MISC, I-...|\n",
      "|[london, 1996-08-30]|[[-0.66074246, 0....|          [B-LOC, O]|\n",
      "|[result, and, clo...|[[-2.2526507, 0.2...|[O, O, O, O, O, O...|\n",
      "|[leicester, :, le...|[[-0.9601011, 0.4...|[B-LOC, O, B-ORG,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.select(\"bert.result\",\"bert.embeddings\",'label.result').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Defining NerDLApproach() Annotator:\n",
    "\n",
    "* setInputCols([“sentence”, “token”, “bert”]) : the columns that will be used by NER model to generate features.\n",
    "* setLabelColumn(“label”) : <span style=\"color:yellow\"> the target columns, where the GROUND TRUTH or Original Label is stored </span>\n",
    "* setOutputCol(“ner”) : the predictions will be written to ner column\n",
    "* setMaxEpochs(1) : number of epoch for training\n",
    "* setVerbose(1) : the level of logs while training\n",
    "* setValidationSplit(0.2) : the proportion of training dataset to be validated against the model on each Epoch. The value should be between 0.0 and 1.0 and by default, it is 0.0 and off.\n",
    "* setEvaluationLogExtended(True) : Whether logs for validation to be extended: it displays time and evaluation of each label. The default is false.\n",
    "* setEnableOutputLogs(True) : Whether to output to the log folder. When set True, the logs and training metrics will be written to folder in the home folder.\n",
    "* setIncludeConfidence(True) : whether to include confidence scores in annotation metadata.\n",
    "* setTestDataset(“test_withEmbeds.parquet”) : <span style=\"color:yellow\">The path to test data. If set, it is used to calculate statistics on it during training</span>. The resulting statistics are recored at each epch at the log file if the setVerbose is 1. This is also in a CoNLL format but embeddings added with bert_annotator and saved to disk as before. You don’t have to set this if you don’t need to evaluate your model on an unseen test set in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jJyP74zgM6OQ"
   },
   "outputs": [],
   "source": [
    "nerTagger = NerDLApproach()\\\n",
    "  .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    "  .setLabelColumn(\"label\")\\\n",
    "  .setOutputCol(\"ner\")\\\n",
    "  .setMaxEpochs(5)\\\n",
    "  .setLr(0.001)\\\n",
    "  .setPo(0.005)\\\n",
    "  .setBatchSize(32)\\\n",
    "  .setEvaluationLogExtended(True) \\\n",
    "  .setEnableOutputLogs(True)\\\n",
    "  .setTestDataset(\"test_withEmbeds.parquet\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    bert_annotator,\n",
    "    nerTagger\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYJT6mHLM6OQ"
   },
   "source": [
    "You can also set learning rate ( setLr ), learning rate decay coefficient ( setPo ), setBatchSize and setDropout rate. Please see the [official APIs](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/ner/dl/NerDLApproach.html) for the entire list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2wCMb1XmM6OR",
    "outputId": "8abd0af8-b298-4a67-d280-a7e607b86d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.5 ms, sys: 99.7 ms, total: 158 ms\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ner_model = pipeline.fit(training_data.limit(1000))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtLbO6RfP-y7",
    "outputId": "75bbd436-7b82-4205-f86e-e07f9194af53"
   },
   "source": [
    "!ls -l /root/annotator_logs/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zAvs21PQDU1",
    "outputId": "e1fa4f56-ac31-4299-e29e-056cda24a139"
   },
   "source": [
    "!cat /root/annotator_logs/NerDLApproach_16a0e7b3577f.log"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "oo-GNidLr1wt"
   },
   "source": [
    "**Some notes:**\n",
    "- we used the smallest BERT model called BERT Tiny\n",
    "- it's very small and requires less memory among Transformers\n",
    "- if you have more memory or access to accelerated hardware please choose a larger BERT model for higher accuracy\n",
    "- you can also set higher Epoch to reach our STOA metrics\n",
    "\n",
    "We chose the smallest BERT model with only 5 Epochs for the sake of this tutorial within this small Colab VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ApUISh0AM6OR"
   },
   "outputs": [],
   "source": [
    "# let's save our trained NER model on disk\n",
    "# so we can load it in a new session or move it to another location\n",
    "# since we fit NerDL model inside the pipeline, we can access it via stages\n",
    "ner_model.stages[1].write().overwrite().save('./NER_bert_20200418')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WehIZ-d9s5Ok",
    "outputId": "26116dd1-b22e-4c7f-85df-cfc68a00eb1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|                bert|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|CRICKET - LEICEST...|[{document, 0, 64...|[{document, 0, 64...|[{token, 0, 6, CR...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|   LONDON 1996-08-30|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 5, LO...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|West Indian all-r...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 3, We...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|Their stay on top...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 4, Th...|[{pos, 0, 4, PRP$...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|After bowling Som...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|Trailing by 213 ,...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 7, Tr...|[{pos, 0, 7, VBG,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|Essex , however ,...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 4, Es...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|Hussain , conside...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 6, Hu...|[{pos, 0, 6, NN, ...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|By the close York...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 1, By...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|At the Oval , Sur...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 1, At...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|He was well backe...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 1, He...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|Derbyshire kept u...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 9, De...|[{pos, 0, 9, NN, ...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|Australian Tom Mo...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 9, Au...|[{pos, 0, 9, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|After the frustra...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 4, Af...|[{pos, 0, 4, IN, ...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|They were held up...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 3, Th...|[{pos, 0, 3, PRP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|By stumps Kent ha...|[{document, 0, 41...|[{document, 0, 41...|[{token, 0, 1, By...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|CRICKET - ENGLISH...|[{document, 0, 45...|[{document, 0, 45...|[{token, 0, 6, CR...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|   LONDON 1996-08-30|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 5, LO...|[{pos, 0, 5, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|Result and close ...|[{document, 0, 81...|[{document, 0, 81...|[{token, 0, 5, Re...|[{pos, 0, 5, NN, ...|[{named_entity, 0...|[{word_embeddings...|\n",
      "|Leicester : Leice...|[{document, 0, 67...|[{document, 0, 67...|[{token, 0, 8, Le...|[{pos, 0, 8, JJ, ...|[{named_entity, 0...|[{word_embeddings...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Evaluation of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiQvE5pmM6OS",
    "outputId": "6264b27c-2399-49a3-f6ac-17c3698e8177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            sentence|               token|               label|                bert|                 ner|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[{document, 0, 64...|[{token, 0, 6, CR...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
      "|[{document, 0, 16...|[{token, 0, 5, LO...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
      "|[{document, 0, 18...|[{token, 0, 3, We...|[{named_entity, 0...|[{word_embeddings...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's only feed sentence and token from our test dataset\n",
    "predictions = ner_model.transform(test_data.select(\"sentence\", \"token\", \"label\"))\n",
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8eTDVoCxM6OS",
    "outputId": "80147911-7888-408e-f963-ac8510db18c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|                                  result|                                  result|                                  result|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|[CRICKET, -, LEICESTERSHIRE, TAKE, OV...|   [O, O, B-ORG, O, O, O, O, O, O, O, O]|       [O, O, O, O, O, O, O, O, O, O, O]|\n",
      "|                    [LONDON, 1996-08-30]|                              [B-LOC, O]|                              [B-LOC, O]|\n",
      "|[West, Indian, all-rounder, Phil, Sim...|[B-MISC, I-MISC, O, B-PER, I-PER, O, ...|[O, B-MISC, O, O, I-PER, O, O, O, O, ...|\n",
      "|[Their, stay, on, top, ,, though, ,, ...|[O, O, O, O, O, O, O, O, O, O, O, O, ...|[O, O, O, O, O, O, O, O, O, O, O, O, ...|\n",
      "|[After, bowling, Somerset, out, for, ...|[O, O, B-ORG, O, O, O, O, O, O, O, O,...|[O, O, I-PER, O, O, O, O, O, O, O, O,...|\n",
      "|[Trailing, by, 213, ,, Somerset, got,...|[O, O, O, O, B-ORG, O, O, O, O, O, O,...|[O, O, O, O, B-PER, O, O, O, O, O, O,...|\n",
      "|[Essex, ,, however, ,, look, certain,...|[B-ORG, O, O, O, O, O, O, O, O, O, O,...|[O, O, O, O, O, O, O, O, O, O, O, O, ...|\n",
      "|[Hussain, ,, considered, surplus, to,...|[B-PER, O, O, O, O, B-LOC, O, O, O, O...|[O, O, O, O, O, B-LOC, O, O, O, O, O,...|\n",
      "|[By, the, close, Yorkshire, had, turn...|[O, O, O, B-ORG, O, O, O, O, O, O, O,...|[O, O, O, O, O, O, O, O, O, O, O, O, ...|\n",
      "|[At, the, Oval, ,, Surrey, captain, C...|[O, O, B-LOC, O, B-ORG, O, B-PER, I-P...|[O, O, O, O, B-PER, B-PER, B-PER, I-P...|\n",
      "|[He, was, well, backed, by, England, ...|[O, O, O, O, O, B-LOC, O, B-PER, I-PE...|[O, O, O, O, O, B-PER, O, B-PER, I-PE...|\n",
      "|[Derbyshire, kept, up, the, hunt, for...|[B-ORG, O, O, O, O, O, O, O, O, O, O,...|[O, O, O, O, O, O, O, O, O, O, O, O, ...|\n",
      "|[Australian, Tom, Moody, took, six, f...|[B-MISC, B-PER, I-PER, O, O, O, O, O,...|[B-PER, B-PER, I-PER, O, O, O, O, O, ...|\n",
      "|[After, the, frustration, of, seeing,...|[O, O, O, O, O, O, O, O, O, O, O, O, ...|[O, O, O, O, O, O, O, O, O, O, O, O, ...|\n",
      "|[They, were, held, up, by, a, gritty,...|[O, O, O, O, O, O, O, O, O, B-PER, I-...|[O, O, O, O, O, O, O, O, O, B-PER, I-...|\n",
      "|[By, stumps, Kent, had, reached, 108,...|         [O, O, B-ORG, O, O, O, O, O, O]|             [O, O, O, O, O, O, O, O, O]|\n",
      "|[CRICKET, -, ENGLISH, COUNTY, CHAMPIO...|    [O, O, B-MISC, I-MISC, I-MISC, O, O]|         [O, O, B-MISC, I-MISC, O, O, O]|\n",
      "|                    [LONDON, 1996-08-30]|                              [B-LOC, O]|                              [B-LOC, O]|\n",
      "|[Result, and, close, of, play, scores...|[O, O, O, O, O, O, O, B-MISC, O, O, O...|[O, O, O, O, O, O, O, B-MISC, O, O, O...|\n",
      "|[Leicester, :, Leicestershire, beat, ...|[B-LOC, O, B-ORG, O, B-ORG, O, O, O, ...|[O, O, B-ORG, O, O, O, O, O, O, O, O, O]|\n",
      "+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('token.result','label.result','ner.result').show(truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzC5CDUwM6OS",
    "outputId": "da65b6e2-6725-4e03-8fff-3c377b0a6e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentence: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- token: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- label: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- bert: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- ner: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzYP3TGfM6OT",
    "outputId": "36490d72-4c60-4649-ce01-f10f3222f3cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------------+\n",
      "|token         |ground_truth|prediction_results|\n",
      "+--------------+------------+------------------+\n",
      "|CRICKET       |O           |O                 |\n",
      "|-             |O           |O                 |\n",
      "|LEICESTERSHIRE|B-ORG       |O                 |\n",
      "|TAKE          |O           |O                 |\n",
      "|OVER          |O           |O                 |\n",
      "|AT            |O           |O                 |\n",
      "|TOP           |O           |O                 |\n",
      "|AFTER         |O           |O                 |\n",
      "|INNINGS       |O           |O                 |\n",
      "|VICTORY       |O           |O                 |\n",
      "|.             |O           |O                 |\n",
      "|LONDON        |B-LOC       |B-LOC             |\n",
      "|1996-08-30    |O           |O                 |\n",
      "|West          |B-MISC      |O                 |\n",
      "|Indian        |I-MISC      |B-MISC            |\n",
      "|all-rounder   |O           |O                 |\n",
      "|Phil          |B-PER       |O                 |\n",
      "|Simmons       |I-PER       |I-PER             |\n",
      "|took          |O           |O                 |\n",
      "|four          |O           |O                 |\n",
      "+--------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
    ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "        F.expr(\"cols['1']\").alias(\"ground_truth\"),\n",
    "        F.expr(\"cols['2']\").alias(\"prediction_results\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjaahMbzuV4A"
   },
   "source": [
    "#### Convert to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "z-M0PZd6M6OU",
    "outputId": "84421d33-68ed-4259-a3f5-7c0a77ca831a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>result</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, T...</td>\n",
       "      <td>[O, O, B-ORG, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[LONDON, 1996-08-30]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[West, Indian, all-rounder, Phil, Simmons, too...</td>\n",
       "      <td>[B-MISC, I-MISC, O, B-PER, I-PER, O, O, O, O, ...</td>\n",
       "      <td>[O, B-MISC, O, O, I-PER, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Their, stay, on, top, ,, though, ,, may, be, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[After, bowling, Somerset, out, for, 83, on, t...</td>\n",
       "      <td>[O, O, B-ORG, O, O, O, O, O, O, O, O, B-LOC, I...</td>\n",
       "      <td>[O, O, I-PER, O, O, O, O, O, O, O, O, B-PER, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>[But, the, prices, may, move, in, a, close, ra...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>[Brokers, said, blue, chips, like, IDLC, ,, Ba...</td>\n",
       "      <td>[O, O, O, O, O, B-ORG, O, B-ORG, I-ORG, O, B-O...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-LOC, O, O, B-LOC, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>[They, said, there, was, still, demand, for, b...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>[The, DSE, all, share, price, index, closed, 2...</td>\n",
       "      <td>[O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>[--, Dhaka, Newsroom, 880-2-506363]</td>\n",
       "      <td>[O, B-ORG, I-ORG, O]</td>\n",
       "      <td>[O, B-LOC, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 result  \\\n",
       "0     [CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, T...   \n",
       "1                                  [LONDON, 1996-08-30]   \n",
       "2     [West, Indian, all-rounder, Phil, Simmons, too...   \n",
       "3     [Their, stay, on, top, ,, though, ,, may, be, ...   \n",
       "4     [After, bowling, Somerset, out, for, 83, on, t...   \n",
       "...                                                 ...   \n",
       "3245  [But, the, prices, may, move, in, a, close, ra...   \n",
       "3246  [Brokers, said, blue, chips, like, IDLC, ,, Ba...   \n",
       "3247  [They, said, there, was, still, demand, for, b...   \n",
       "3248  [The, DSE, all, share, price, index, closed, 2...   \n",
       "3249                [--, Dhaka, Newsroom, 880-2-506363]   \n",
       "\n",
       "                                                 result  \\\n",
       "0                 [O, O, B-ORG, O, O, O, O, O, O, O, O]   \n",
       "1                                            [B-LOC, O]   \n",
       "2     [B-MISC, I-MISC, O, B-PER, I-PER, O, O, O, O, ...   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG,...   \n",
       "4     [O, O, B-ORG, O, O, O, O, O, O, O, O, B-LOC, I...   \n",
       "...                                                 ...   \n",
       "3245   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3246  [O, O, O, O, O, B-ORG, O, B-ORG, I-ORG, O, B-O...   \n",
       "3247  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3248  [O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "3249                               [O, B-ORG, I-ORG, O]   \n",
       "\n",
       "                                                 result  \n",
       "0                     [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1                                            [B-LOC, O]  \n",
       "2     [O, B-MISC, O, O, I-PER, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4     [O, O, I-PER, O, O, O, O, O, O, O, O, B-PER, O...  \n",
       "...                                                 ...  \n",
       "3245   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "3246  [O, O, O, O, O, O, O, B-LOC, O, O, B-LOC, O, O...  \n",
       "3247  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3248  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3249                                   [O, B-LOC, O, O]  \n",
       "\n",
       "[3250 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = predictions.select('token.result','label.result','ner.result').toPandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N47oS8WM6OY"
   },
   "source": [
    "### 2.6 Building Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2ZMRW44DM6OY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L2_128 download started this may take some time.\n",
      "Approximate size to download 16.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "bert = BertEmbeddings.pretrained('small_bert_L2_128', 'en') \\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"bert\")\\\n",
    " .setCaseSensitive(True)\n",
    "\n",
    "loaded_ner_model = NerDLModel.load(\"NER_bert_20200418\")\\\n",
    " .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    " .setOutputCol(\"ner\")\n",
    "\n",
    "converter = NerConverter()\\\n",
    "  .setInputCols([\"document\", \"token\", \"ner\"])\\\n",
    "  .setOutputCol(\"ner_span\")\n",
    "\n",
    "ner_prediction_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        sentence,\n",
    "        token,\n",
    "        bert,\n",
    "        loaded_ner_model, # User tuned pre-trained bert model \n",
    "        converter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Lmtzkvh0M6OZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|text|\n",
      "+----+\n",
      "|    |\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "empty_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "b-ntORrdM6OZ"
   },
   "outputs": [],
   "source": [
    "prediction_model = ner_prediction_pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "L6hxHQBLM6OZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Peter Parker is a...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Peter Parker is a nice guy and lives in New York.\"\n",
    "sample_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "sample_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Os-XoJCtM6Oa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|                text|            document|            sentence|               token|                bert|                 ner|ner_span|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|Peter Parker is a...|[{document, 0, 48...|[{document, 0, 48...|[{token, 0, 4, Pe...|[{word_embeddings...|[{named_entity, 0...|      []|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = prediction_model.transform(sample_data)\n",
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "flcEnI6BM6Oa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(result=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.select('ner.result').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "95ryt1wEM6Oa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|chunk|entity|\n",
      "+-----+------+\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.select(F.explode(F.arrays_zip(\"ner_span.result\",\"ner_span.metadata\")).alias(\"entities\")) \\\n",
    ".select(F.expr(\"entities['0']\").alias(\"chunk\"),\n",
    "        F.expr(\"entities['1'].entity\").alias(\"entity\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building NER Pipeline with Glove Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbY3ZstAM6OV"
   },
   "source": [
    "### 3.1 NER Model with Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "40YZFzD6M6OW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "glove = WordEmbeddingsModel().pretrained() \\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"glove\")\\\n",
    " .setCaseSensitive(False)\n",
    "\n",
    "test_data = CoNLL().readDataset(spark, './eng.testa')\n",
    "test_data = glove.transform(test_data.limit(1000))\n",
    "test_data.write.parquet(\"test_withGloveEmbeds.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ygjYFiZMM6OW"
   },
   "outputs": [],
   "source": [
    "nerTagger.setInputCols([\"sentence\", \"token\", \"glove\"])\n",
    "nerTagger.setTestDataset(\"test_withGloveEmbeds.parquet\")\n",
    "\n",
    "glove_pipeline = Pipeline(\n",
    "    stages = [\n",
    "    glove,\n",
    "    nerTagger\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "JJLeqlq3M6OW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.5 ms, sys: 52.7 ms, total: 101 ms\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ner_model_v3 = glove_pipeline.fit(training_data.limit(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ApUISh0AM6OR"
   },
   "outputs": [],
   "source": [
    "# let's save our trained NER model on disk\n",
    "# so we can load it in a new session or move it to another location\n",
    "# since we fit NerDL model inside the pipeline, we can access it via stages\n",
    "ner_model_v3.stages[1].write().overwrite().save('./NER_glove_20200418')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ix0O0YPwM6OX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+----------+\n",
      "|token         |ground_truth|prediction|\n",
      "+--------------+------------+----------+\n",
      "|CRICKET       |O           |O         |\n",
      "|-             |O           |O         |\n",
      "|LEICESTERSHIRE|B-ORG       |B-ORG     |\n",
      "|TAKE          |O           |O         |\n",
      "|OVER          |O           |O         |\n",
      "|AT            |O           |O         |\n",
      "|TOP           |O           |O         |\n",
      "|AFTER         |O           |O         |\n",
      "|INNINGS       |O           |O         |\n",
      "|VICTORY       |O           |O         |\n",
      "|.             |O           |O         |\n",
      "|LONDON        |B-LOC       |B-LOC     |\n",
      "|1996-08-30    |O           |O         |\n",
      "|West          |B-MISC      |B-LOC     |\n",
      "|Indian        |I-MISC      |O         |\n",
      "|all-rounder   |O           |O         |\n",
      "|Phil          |B-PER       |B-PER     |\n",
      "|Simmons       |I-PER       |I-PER     |\n",
      "|took          |O           |O         |\n",
      "|four          |O           |O         |\n",
      "+--------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_v3 = ner_model_v3.transform(test_data.limit(10))\n",
    "\n",
    "# test_data.sample(False,0.1,0)\n",
    "predictions_v3.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
    ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "        F.expr(\"cols['1']\").alias(\"ground_truth\"),\n",
    "        F.expr(\"cols['2']\").alias(\"prediction\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using glove embeddings pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QT2ojwJHM6Oa"
   },
   "outputs": [],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# The original code loads NER_bert_20200219\n",
    "loaded_ner_model = NerDLModel.load(\"NER_glove_20200418\")\\\n",
    " .setInputCols([\"sentence\", \"token\", \"glove\"])\\\n",
    " .setOutputCol(\"ner\")\n",
    "\n",
    "converter = NerConverter()\\\n",
    "  .setInputCols([\"document\", \"token\", \"ner\"])\\\n",
    "  .setOutputCol(\"ner_span\")\n",
    "\n",
    "glove_ner_prediction_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        sentence,\n",
    "        token,\n",
    "        glove,\n",
    "        loaded_ner_model,\n",
    "        converter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Lmtzkvh0M6OZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|text|\n",
      "+----+\n",
      "|    |\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "empty_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Iw71QpnXM6Ob"
   },
   "outputs": [],
   "source": [
    "glove_prediction_model = glove_ner_prediction_pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "L6hxHQBLM6OZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Peter Parker is a...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Peter Parker is a nice guy and lives in New York.\"\n",
    "sample_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "sample_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "lqCcz7rwM6Ob"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|               glove|                 ner|            ner_span|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Peter Parker is a...|[{document, 0, 48...|[{document, 0, 48...|[{token, 0, 4, Pe...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 0, 11, P...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = glove_prediction_model.transform(sample_data)\n",
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "DsoWGO7MM6Ob"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|chunk       |entity|\n",
      "+------------+------+\n",
      "|Peter Parker|PER   |\n",
      "|York        |LOC   |\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.select(F.explode(F.arrays_zip(\"ner_span.result\",\"ner_span.metadata\")).alias(\"entities\")) \\\n",
    ".select(F.expr(\"entities['0']\").alias(\"chunk\"),\n",
    "        F.expr(\"entities['1'].entity\").alias(\"entity\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYWIys6BM6Ob"
   },
   "source": [
    "## 4. Pretrained NER Pipelines\n",
    "### 4.1. Pretrained pipeline: \"recognize_entities_dl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "nOP1riCCM6Oc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recognize_entities_dl download started this may take some time.\n",
      "Approx size to download 160.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "pretrained_pipeline = PretrainedPipeline('recognize_entities_dl', lang='en')\n",
    "\n",
    "#onto_recognize_entities_sm\n",
    "#explain_document_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "tdh4U83kM6Oc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Peter', 'B-PER'),\n",
       " ('Parker', 'I-PER'),\n",
       " ('is', 'O'),\n",
       " ('a', 'O'),\n",
       " ('nice', 'O'),\n",
       " ('guy', 'O'),\n",
       " ('and', 'O'),\n",
       " ('lives', 'O'),\n",
       " ('in', 'O'),\n",
       " ('New', 'B-LOC'),\n",
       " ('York', 'I-LOC'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = \"The Mona Lisa is a 16th century oil painting created by Leonardo. It's held at the Louvre in Paris.\"\n",
    "text = \"Peter Parker is a nice guy and lives in New York.\"\n",
    "result = pretrained_pipeline.annotate(text)\n",
    "\n",
    "list(zip(result['token'], result['ner']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Pretrained pipeline: \"explain_document_dl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "M-F-QJzCM6Oc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_dl download started this may take some time.\n",
      "Approx size to download 169.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "pretrained_pipeline2 = PretrainedPipeline('explain_document_dl', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Da9Ql6-hM6Oc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Peter', 'Peter', 'NNP', 'B-PER', 'Peter', 'peter'),\n",
       " ('Parker', 'Parker', 'NNP', 'I-PER', 'Parker', 'parker'),\n",
       " ('is', 'is', 'VBZ', 'O', 'be', 'i'),\n",
       " ('a', 'a', 'DT', 'O', 'a', 'a'),\n",
       " ('nice', 'nice', 'JJ', 'O', 'nice', 'nice'),\n",
       " ('guy', 'guy', 'NN', 'O', 'guy', 'gui'),\n",
       " ('and', 'and', 'CC', 'O', 'and', 'and'),\n",
       " ('lives', 'lives', 'NNS', 'O', 'life', 'live'),\n",
       " ('in', 'in', 'IN', 'O', 'in', 'in'),\n",
       " ('New', 'New', 'NNP', 'B-LOC', 'New', 'new'),\n",
       " ('York', 'York', 'NNP', 'I-LOC', 'York', 'york'),\n",
       " ('.', '.', '.', 'O', '.', '.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = \"The Mona Lisa is a 16th centry oil painting created by Leonrdo. It's held at the Louvre in Paris.\"\n",
    "text = \"Peter Parker is a nice guy and lives in New York.\"\n",
    "\n",
    "result2 = pretrained_pipeline2.annotate(text)\n",
    "result2\n",
    "list(zip(result2['token'],  result2['checked'], result2['pos'], result2['ner'],  result2['lemma'],  result2['stem']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "UotW3Nr_M6Od"
   },
   "source": [
    "xx= pretrained_pipeline2.fullAnnotate(text)\n",
    "\n",
    "[(n.result, n.metadata['entity']) for n in xx['ner_span']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison of NER models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "26z4CW7QM6OX"
   },
   "source": [
    "np.array (predictions.select('token.result').take(1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "yGZjA_pBM6OX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>ground</th>\n",
       "      <th>label_bert_0</th>\n",
       "      <th>label_glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRICKET</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LEICESTERSHIRE</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TAKE</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OVER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TOP</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AFTER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INNINGS</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VICTORY</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             token ground label_bert_0 label_glove\n",
       "0          CRICKET      O            O           O\n",
       "1                -      O            O           O\n",
       "2   LEICESTERSHIRE  B-ORG            O       B-ORG\n",
       "3             TAKE      O            O           O\n",
       "4             OVER      O            O           O\n",
       "5               AT      O            O           O\n",
       "6              TOP      O            O           O\n",
       "7            AFTER      O            O           O\n",
       "8          INNINGS      O            O           O\n",
       "9          VICTORY      O            O           O\n",
       "10               .      O            O           O"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tokens = np.array (predictions.select('token.result').take(1))[0][0]\n",
    "ground = np.array (predictions.select('label.result').take(1))[0][0]\n",
    "label_bert_0 = np.array (predictions.select('ner.result').take(1))[0][0]\n",
    "#label_bert_2 = np.array (predictions_v2.select('ner.result').take(1))[0][0]\n",
    "label_glove = np.array (predictions_v3.select('ner.result').take(1))[0][0]\n",
    "\n",
    "pd.DataFrame({'token':tokens, 'ground':ground, 'label_bert_0':label_bert_0, 'label_glove':label_glove})\n",
    "              #'label_bert_2':label_bert_2,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwAHj6mPM6Od"
   },
   "source": [
    "## 6. Using your own custom Word Embedding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "bC8RR7iXM6Od"
   },
   "source": [
    "custom_embeddings = WordEmbeddings()\\\n",
    "  .setInputCols([\"sentence\", \"token\"])\\\n",
    "  .setOutputCol(\"glove\")\\\n",
    "  .setStoragePath('/Users/vkocaman/cache_pretrained/PubMed-shuffle-win-2.bin', \"BINARY\")\\\n",
    ".setDimension(200)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "rVkJgZnRM6Od",
    "outputId": "51edbce3-679e-4626-95ab-c3070f17e533"
   },
   "source": [
    "custom_embeddings.fit(training_data.limit(10)).transform(training_data.limit(10)).show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "YKp_pARIM6Oe"
   },
   "source": [
    "## 7. Creating your own CoNLL dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "kdCxFU2HM6Oe"
   },
   "source": [
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "def get_ann_pipeline ():\n",
    "    \n",
    "    document_assembler = DocumentAssembler() \\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol('document')\n",
    "\n",
    "    sentence = SentenceDetector()\\\n",
    "        .setInputCols(['document'])\\\n",
    "        .setOutputCol('sentence')\\\n",
    "        .setCustomBounds(['\\n'])\n",
    "\n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols([\"sentence\"]) \\\n",
    "        .setOutputCol(\"token\")\n",
    "\n",
    "    pos = PerceptronModel.pretrained() \\\n",
    "              .setInputCols([\"sentence\", \"token\"]) \\\n",
    "              .setOutputCol(\"pos\")\n",
    "    \n",
    "    embeddings = WordEmbeddingsModel.pretrained()\\\n",
    "          .setInputCols([\"sentence\", \"token\"])\\\n",
    "          .setOutputCol(\"embeddings\")\n",
    "\n",
    "    ner_model = NerDLModel.pretrained() \\\n",
    "          .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "          .setOutputCol(\"ner\")\n",
    "\n",
    "    ner_converter = NerConverter()\\\n",
    "      .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n",
    "      .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "    ner_pipeline = Pipeline(\n",
    "        stages = [\n",
    "            document_assembler,\n",
    "            sentence,\n",
    "            tokenizer,\n",
    "            pos,\n",
    "            embeddings,\n",
    "            ner_model,\n",
    "            ner_converter\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "    ner_pipelineFit = ner_pipeline.fit(empty_data)\n",
    "\n",
    "    ner_lp_pipeline = LightPipeline(ner_pipelineFit)\n",
    "\n",
    "    print (\"Spark NLP NER lightpipeline is created\")\n",
    "\n",
    "    return ner_lp_pipeline\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "IMgY0pEhM6Oe",
    "outputId": "680f7dfb-e9ae-43d2-c478-6761d61d7cf6"
   },
   "source": [
    "conll_pipeline = get_ann_pipeline ()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "4aYv-WM1M6Oe",
    "outputId": "f3fdf133-4750-4a67-b2f4-5540379c4b72"
   },
   "source": [
    "parsed = conll_pipeline.annotate (\"Peter Parker is a nice guy and lives in New York.\")\n",
    "parsed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "nNpOPXUSM6Of",
    "outputId": "b24a0cee-31a9-42d3-c8e3-19876d929f9c"
   },
   "source": [
    "conll_lines=''\n",
    "\n",
    "for token, pos, ner in zip(parsed['token'],parsed['pos'],parsed['ner']):\n",
    "\n",
    "    conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, ner)\n",
    "\n",
    "\n",
    "print(conll_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5N47oS8WM6OY",
    "zYWIys6BM6Ob",
    "WbY3ZstAM6OV",
    "WwAHj6mPM6Od",
    "YKp_pARIM6Oe"
   ],
   "name": "3.NER_with_BERT-start.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
